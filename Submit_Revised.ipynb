{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Submit_Revised.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1bei8T2HE69y",
        "outputId": "53100440-5b5b-4c7b-b272-c62274e82244",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import nltk\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
        "import pandas as pd\n",
        "import itertools\n",
        "import copy\n",
        "from langdetect import detect\n",
        "from langdetect import detect_langs\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from textblob import TextBlob\n",
        "import json\n",
        "import torch.nn as nn\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "stemmer = nltk.stem.PorterStemmer()\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EowGhT2qxkqy",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1m5G_nVmqJMy",
        "outputId": "bc66eae8-6254-49bc-83c1-541f4a8d6cd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pip install langdetect"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.6/dist-packages (1.0.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EtD3lfZkGZi7",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(4)\n",
        "np.random.seed(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EY_M9cSGF0wg",
        "colab": {}
      },
      "source": [
        "x = lambda a,b : list(itertools.chain(a, b))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "17T66-JZJGwQ",
        "colab": {}
      },
      "source": [
        "empty = lambda empty: np.nan if empty=='' else empty"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5Rade5gAFIAm",
        "colab": {}
      },
      "source": [
        "class ConvolutionalNetwork(nn.Module): #the classification net\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.stream1_conv = nn.Conv2d(1, 32, (3,5), 1)  # (input filters, output filters, kernel size, stride)\n",
        "        self.stream2_conv = nn.Conv2d(1, 32, (4,5), 1)\n",
        "        self.stream3_conv = nn.Conv2d(1, 32,(5,5), 1)\n",
        "        \n",
        " \n",
        "        self.fc1 = nn.Linear(32*514+32*513+32*512, 1)   # Note this is specific\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, X):\n",
        "        \n",
        "        X1 = F.relu(self.stream1_conv(X))\n",
        "        X2 = F.relu(self.stream2_conv(X))\n",
        "        X3 = F.relu(self.stream3_conv(X))\n",
        "        \n",
        "        \n",
        "        #X1 = F.max_pool2d(X1, 5, 3)\n",
        "        #X2 = F.max_pool2d(X2, 5, 4)\n",
        "        #X3 = F.max_pool2d(X3, 5, 5)\n",
        "        \n",
        "        X1 = X1.view(-1, 32*514*1)\n",
        "        X2 = X2.view(-1, 32*513*1)\n",
        "        X3 = X3.view(-1, 32*512*1)\n",
        "        \n",
        "        X_ = torch.cat((X1, X2),1)\n",
        "        X_ = torch.cat((X_, X3),1)\n",
        "        X_ = self.fc1(X_)\n",
        "        return self.sigmoid(X_)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4o4nFTvlZRM6",
        "colab": {}
      },
      "source": [
        "class SkimGram(nn.Module):#the embedding-representation formation net\n",
        "    def __init__(self, emb_szs, vocabulary_size):\n",
        "        super().__init__()\n",
        "        self.u = nn.Linear(vocabulary_size,emb_szs,)\n",
        "        self.v = nn.Linear(emb_szs,vocabulary_size)\n",
        "    \n",
        "    def forward(self,x):\n",
        "        x= self.u(x)\n",
        "        x =  self.v(x)\n",
        "        log_softmax = F.log_softmax(x, dim=0)\n",
        "        return log_softmax\n",
        "    \n",
        "    def representation(self,x): #obtain embedding\n",
        "        x=self.u(x)\n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7f7-HEY8FSVN",
        "colab": {}
      },
      "source": [
        "#Perform all relevant preprocessing of the data from natural language perspective and \n",
        "class Preprocess(object):\n",
        "    def __init__(self,threshold=0.7):\n",
        "        self.df = pd.DataFrame({'A' : [np.nan]})\n",
        "        self.df.dropna(inplace=True)\n",
        "        self.threshold=threshold\n",
        "        self.df_values = pd.DataFrame({'A' : [np.nan]})\n",
        "        self.df_values.dropna(inplace=True)\n",
        "\n",
        "\n",
        "        self.idx_pairs = []\n",
        "        self.vocabulary = [1]\n",
        "        self.idx_pairs_Doc = []\n",
        "        self.new_data=[]\n",
        "\n",
        "    def Reading(self,input_file):\n",
        "        #read from input file data set and re-format for model input\n",
        "        with open(input_file, encoding=\"utf8\") as f:\n",
        "            \n",
        "            data = f.readlines()\n",
        "            data = [json.loads(line) for line in data] #convert string to dictionary format\n",
        "\n",
        "        object_value = [i['object'] for i in data]#first category in the data , this is related to the subject of the patent, such as title, summary etc\n",
        "        actor_value = [i['actor'] for i in data]#second category in the data, that is the authors and related detailes\n",
        "        df_objects= pd.DataFrame(object_value,columns='filingDate id objectType publicationDate publicationNumber status summary title'.split()) #convert data to pandas series\n",
        "\n",
        "        actors=[]\n",
        "        dictionar={}\n",
        "        check=[]\n",
        "        temp=[] #convert actor data to pandas series\n",
        "        for actor in actor_value:\n",
        "\n",
        "            appending_location = [i['location']['displayName'] for i in actor]\n",
        "\n",
        "            appending_location = \"|\".join(appending_location)\n",
        "\n",
        "            appending_name = [i['name'] for i in actor]\n",
        "            check=x(check,appending_name)\n",
        "            appending_name = \"|\".join(appending_name)\n",
        "\n",
        "            dictionar['name']=appending_name\n",
        "            dictionar['location']=appending_location\n",
        "            temp.append(dictionar)\n",
        "            actors.append(copy.copy(temp[0]))\n",
        "\n",
        "        df_actors= pd.DataFrame(actors,columns='name location'.split())\n",
        "        #define outputs bases on Patent=1 and anything else as 0, as asked from the question\n",
        "        df_objects.at[df_objects['status']=='Patented Case', 'status'] = 1\n",
        "        df_objects.at[df_objects['status']!=1, 'status'] = 0\n",
        "        #foreign languages, in this case French\n",
        "        df_objects.at[:, 'summary'] =df_objects['summary'].apply(self.language_filter)\n",
        "        #Obtain key dates from submission date, note publication data and number are not considered since they are marked after the patent has been classified\n",
        "        df_objects['filingDate'] = df_objects['filingDate'].apply(self.timestamp)\n",
        "\n",
        "        if self.df.empty: #for the initial data set\n",
        "            self.df=pd.concat( [df_objects , df_actors], axis=1)\n",
        "          #self.df.drop('A',axis=1,inplace = True)\n",
        "        else:\n",
        "            \n",
        "           #for the case of storing additional data then simply concatenate them to the old data\n",
        "            df = pd.concat( [df_objects , df_actors], axis=1)\n",
        "            self.df = pd.concat( [self.df , df], axis=0)\n",
        "        #create the patent document from all the input data\n",
        "        self.merge_data()\n",
        "    \n",
        "    def language_filter(self,corpus):\n",
        "        #remove any French language from the text\n",
        "        if corpus=='':\n",
        "            return ''\n",
        "        qq=detect_langs(corpus)\n",
        "        if qq[0].lang=='en':\n",
        "            if qq[0].prob>0.9:\n",
        "                return corpus\n",
        "            else:\n",
        "                ENcorpus = corpus.split('(FR)')[0]\n",
        "                return ENcorpus\n",
        "        else:\n",
        "\n",
        "            return corpus.split('(FR)')[0]\n",
        "      \n",
        "    def timestamp(self,times): #obtain month day and year\n",
        "        \n",
        "        if type(times)==str:\n",
        "            if times=='':\n",
        "                return ''\n",
        "\n",
        "            tmp = times.split()\n",
        "\n",
        "            pq = tmp[1] + ' ' + tmp[2] + ', ' + tmp[-1] \n",
        "            return pq\n",
        "        else:\n",
        "            return np.nan\n",
        "      \n",
        "    def merge_data(self,flag=0):\n",
        "    #creates patent document (the entire input to the model), flag=0 for the starting data, flag=1 for new data \n",
        "        if flag==0:#pre-processing for the case of retraining\n",
        "            self.df_values = pd.DataFrame({'A' : [np.nan]})\n",
        "            self.df_values.dropna(inplace=True)\n",
        "            columns=['id','filingDate','location','name','title','summary']\n",
        "            merge_data= self.df[columns.pop(0)]\n",
        "            for i in columns:\n",
        "                merge_data = merge_data + ' | ' + self.df[i] #merge each piece of information using a vertical column as seperator\n",
        "\n",
        "            merge_data = pd.concat([merge_data, self.df['status']], axis=1)\n",
        "            merge_data.columns= ['x_input', 'y_output']\n",
        "\n",
        "            if self.df_values.empty:\n",
        "                self.df_values=merge_data\n",
        "\n",
        "            else:\n",
        "                self.df_values = pd.concat( [self.df_values , merge_data], axis=1)\n",
        "        else: #pre-processing for the case of prediction\n",
        "\n",
        "            columns=['id','filingDate','location','name','title','summary']\n",
        "            merge_data= self.new_data[columns.pop(0)]\n",
        "            for i in columns:\n",
        "                merge_data = merge_data + ' | ' + self.new_data[i]\n",
        "              #for the case of prediction there is no output label hence only need to encode in case of validation based on new data\n",
        "            if 'status' in self.new_data.columns:\n",
        "                merge_data = pd.concat([merge_data, self.new_data['status']], axis=1)\n",
        "                merge_data.columns= ['x_input', 'y_output']\n",
        "            else:\n",
        "                merge_data.columns= ['x_input']\n",
        "            return merge_data\n",
        "\n",
        "    def preprocess_input(self,entries=[],flag=0 ):\n",
        "        \n",
        "        #create tokens and vocabulary out of the input data and then encode the text data for the skim gram and classifier net\"\"\"\n",
        "        if flag==0:\n",
        "            entries = self.df_values['x_input']\n",
        "\n",
        "        token_entries = [nltk.word_tokenize(str(entry)) for entry in self.df_values['x_input']]\n",
        "\n",
        "        clean_token_entries = [[word for word in token_entry if word.lower() not in  stopwords.words('english')] for token_entry in token_entries]\n",
        "        window_range=1\n",
        "        vocabulary = []\n",
        "\n",
        "        for entry in clean_token_entries:\n",
        "            for token in entry:\n",
        "                if token not in vocabulary:\n",
        "                    vocabulary.append(token)\n",
        "\n",
        "        vocabulary.append('<e>')#this is a padding token in order to make each document have the same input size\n",
        "        word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
        "        idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
        "\n",
        "        vocabulary_size = len(vocabulary)\n",
        "        window_size = 2\n",
        "        idx_pairs = []\n",
        "        idx_pairs_Doc=[]\n",
        "        for entry in clean_token_entries:\n",
        "            indices = [word2idx[w] for w in entry]\n",
        "            indices_Doc = []\n",
        "\n",
        "            for center_word in range(len(indices)):\n",
        "                for window in range(-window_range,window_range+1):\n",
        "                    check=center_word+window\n",
        "\n",
        "                    if check < 0 or check > len(indices)-1 or check == center_word:\n",
        "                        continue\n",
        "\n",
        "                    context_word_idx = indices[center_word]\n",
        "                    idx_pairs.append((indices[check], context_word_idx))\n",
        "                    indices_Doc.append([indices[check], context_word_idx])\n",
        "            idx_pairs_Doc.append(indices_Doc)\n",
        "\n",
        "        idx_pairs.append((word2idx['<e>'],word2idx['<e>']))       \n",
        "        idx_pairs = np.array(idx_pairs)\n",
        "\n",
        "\n",
        "        lengths = [len(i) for i in np.array(idx_pairs_Doc)]\n",
        "        padding_token = [word2idx['<e>'],word2idx['<e>']]\n",
        "        self.padding_token = padding_token\n",
        "        for i in range(len(idx_pairs_Doc)):\n",
        "            copies = [padding_token]*(max(lengths)-len(idx_pairs_Doc[i]))\n",
        "\n",
        "            idx_pairs_Doc[i] = idx_pairs_Doc[i]+copies\n",
        "\n",
        "            idx_pairs_Doc[i] = [np.array(tuple(k)) for k in idx_pairs_Doc[i]]\n",
        "\n",
        "        idx_pairs_Doc = np.array(idx_pairs_Doc)\n",
        "        if flag==0:\n",
        "            self.idx_pairs = idx_pairs #for skim gram model\n",
        "            self.vocabulary = vocabulary\n",
        "            self.idx_pairs_Doc = idx_pairs_Doc#for classification model\n",
        "        else:\n",
        "            self.idx_pairs_new = idx_pairs #for skim gram model\n",
        "            self.vocabulary_new = vocabulary\n",
        "            self.idx_pairs_Doc_new = idx_pairs_Doc #for classification model\n",
        "    def get_input_layer(self,word_idx):\n",
        "        #convert to one hot vector based on index on the vocabulary\"\n",
        "        x = torch.zeros(len(self.vocabulary)).float()\n",
        "        x[int(word_idx.data.numpy()[0])] = 1.0\n",
        "        #x[int(word_idx)] = 1.0\n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bG85wyh9dzT2",
        "colab": {}
      },
      "source": [
        "class DataPrep(Preprocess):\n",
        "    def __init__(self):\n",
        "        Preprocess.__init__(self)\n",
        "\n",
        "    def Segment(self):\n",
        "        #Test-train data set split for skim-gram \n",
        "        X =  np.random.permutation(self.idx_pairs)\n",
        "        #y = self.df_values['y_output'].values\n",
        "\n",
        "        test_set = round(len(X)/5)\n",
        "        self.X_test = X[:test_set]\n",
        "        self.X_train = X[test_set+1::]\n",
        "\n",
        "        self.input_Train = []\n",
        "        self.output_Train = [] \n",
        "        for (data, target)in self.X_train:\n",
        "            self.input_Train.append(data)\n",
        "            self.output_Train.append(target)\n",
        "\n",
        "        self.input_Train.append(self.padding_token[0]) \n",
        "        self.output_Train.append(self.padding_token[1])\n",
        "        self.input_Test = []\n",
        "        self.output_Test = [] \n",
        "        for (data, target)in self.X_test:\n",
        "            self.input_Test.append(data)\n",
        "            self.output_Test.append(target)\n",
        "        #Test-train data set split for convolution net  \n",
        "        temp=[]\n",
        "        concant=[]\n",
        "        y = np.array(self.df_values['y_output'])\n",
        "        \n",
        "        for i in range(0,len(y)):\n",
        "            concant.append(self.idx_pairs_Doc[i])\n",
        "            concant.append(y[i])\n",
        "            temp.append(concant)\n",
        "            concant=[]\n",
        "        \n",
        "        qq =  np.random.permutation(temp)\n",
        "\n",
        "        X_conv=[]\n",
        "        y_conv=[]\n",
        "        for i in qq: \n",
        "            X_conv.append(i[0])\n",
        "            y_conv.append(i[1])\n",
        "        \n",
        "        X_conv = np.array(X_conv)\n",
        "        y_conv = np.array(y_conv)\n",
        "        #X_conv = self.idx_pairs_Doc\n",
        "        #y_conv = self.df_values['y_output']\n",
        "\n",
        "\n",
        "        test_set = round(len(X_conv)/5)\n",
        "        self.X_test_conv = X_conv[:test_set]\n",
        "        self.y_test_conv = y_conv[:test_set]\n",
        "\n",
        "        self.X_train_conv = X_conv[test_set+1::]\n",
        "        self.y_train_conv = y_conv[test_set+1::]\n",
        "\n",
        "        self.input_Train_conv = []\n",
        "        self.output_Train_conv = [] \n",
        "        \n",
        "        for (data, target)in zip(*(self.X_train_conv, self.y_train_conv)):\n",
        "            self.input_Train_conv.append(data)\n",
        "            self.output_Train_conv.append(target)\n",
        "        \n",
        "        self.input_Test_conv = []\n",
        "        self.output_Test_conv = [] \n",
        "        for (data, target)in zip(*(self.X_test_conv, self.y_test_conv)):\n",
        "            self.input_Test_conv.append(data)\n",
        "            self.output_Test_conv.append(target)\n",
        "        \n",
        "    def install_data(self,input_data):\n",
        "        #Adds additional data to the original pile of data\n",
        "        #Json file input is implied, labels are to be included there as well\n",
        "        self.Reading(input_data)\n",
        "\n",
        "    def New_Data_Read(self,input_file,FLAG=0):\n",
        "        \n",
        "        #this is for predicting or validating on a new data set\n",
        "        #first insert the data set from json file and then preprocess the data, similar to reading data but modified for prediction as well\n",
        "        self.new_data=[]\n",
        "\n",
        "        with open(input_file, encoding=\"utf8\") as f:\n",
        "\n",
        "            data = f.readlines()\n",
        "            data = [json.loads(line) for line in data] #convert string to dict format\n",
        "\n",
        "        object_value = [i['object'] for i in data]\n",
        "        actor_value = [i['actor'] for i in data]\n",
        "        df_objects= pd.DataFrame(object_value,columns='filingDate id objectType publicationDate publicationNumber status summary title'.split())\n",
        "\n",
        "        actors=[]\n",
        "        dictionar={}\n",
        "        check=[]\n",
        "        temp=[]\n",
        "        for actor in actor_value:\n",
        "\n",
        "            appending_location = [i['location']['displayName'] for i in actor]\n",
        "\n",
        "            appending_location = \"|\".join(appending_location)\n",
        "\n",
        "            appending_name = [i['name'] for i in actor]\n",
        "            check=x(check,appending_name)\n",
        "            appending_name = \"|\".join(appending_name)\n",
        "\n",
        "            dictionar['name'] = appending_name\n",
        "            dictionar['location'] = appending_location\n",
        "            temp.append(dictionar)\n",
        "            actors.append(copy.copy(temp[0]))\n",
        "\n",
        "        df_actors= pd.DataFrame(actors,columns='name location'.split())\n",
        "        if FLAG==0:#validation case\n",
        "            df_objects.at[df_objects['status']=='Patented Case', 'status'] = 1\n",
        "            df_objects.at[df_objects['status']!=1, 'status'] = 0\n",
        "        else:#prediction case\n",
        "            if 'status' in df_objects.columns:\n",
        "                df_objects.drop('status',axis=1,inplace = True)\n",
        "\n",
        "        df_objects.at[:, 'summary'] =df_objects['summary'].apply(self.language_filter)\n",
        "        df_objects['filingDate'] = df_objects['filingDate'].apply(self.timestamp)\n",
        "        self.new_data=pd.concat( [df_objects , df_actors], axis=1)\n",
        "        merged_data = self.merge_data(flag=1)\n",
        "\n",
        "        self.preprocess_input(merged_data,flag=1)\n",
        "        if 'y_output' in merged_data.columns:\n",
        "            self.y_values = merged_data['y_output']\n",
        "            self.segment_new_data(flag=1) #validation case\n",
        "        else:\n",
        "            self.segment_new_data(flag=0) #prediction case\n",
        "\n",
        "    def segment_new_data(self,flag=1):\n",
        "     #Test-train data set split for convolution net  \n",
        "        temp=[]\n",
        "        concant=[]\n",
        "        if flag==1: #validation\n",
        "            y = np.array( self.y_values)\n",
        "\n",
        "            for i in range(len(y)):\n",
        "                concant.append(self.idx_pairs_Doc_new[i])\n",
        "                concant.append(y[i])\n",
        "                temp.append(concant)\n",
        "\n",
        "            qq =  np.random.permutation(temp)\n",
        "\n",
        "            X_new=[]\n",
        "            y_new=[]\n",
        "            for i in qq: \n",
        "                X_new.append(i[0])\n",
        "                y_new.append(i[1])\n",
        "            self.X_new=[]\n",
        "            self.y_new=[]\n",
        "            for (data, target)in zip(*(X_new, y_new)):\n",
        "                self.X_new.append(data)\n",
        "                self.y_new.append(target)\n",
        "            self.X_new = np.array(X_new)\n",
        "            self.y_new = np.array(y_new)\n",
        "        else:#prediction\n",
        "            y = np.array( self.idx_pairs_Doc_new)\n",
        "\n",
        "            for i in range(len(y)):\n",
        "                concant.append(self.idx_pairs_Doc_new[i])\n",
        "                temp.append(concant)\n",
        "\n",
        "            qq =  np.random.permutation(temp)\n",
        "\n",
        "            X_new=[]\n",
        "            for i in qq: \n",
        "                X_new.append(i)\n",
        "            self.X_new=[]\n",
        "            for data in X_new:\n",
        "                self.X_new.append(data)\n",
        "\n",
        "            self.X_new = np.array(X_new)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AUhiym1AuWFA",
        "colab": {}
      },
      "source": [
        "class Method(DataPrep):\n",
        "    \n",
        "    def __init__(self,embedding_dims = 5):\n",
        "        DataPrep.__init__(self)\n",
        "        self.embedding_dims=embedding_dims\n",
        "\n",
        "    def define_models(self):\n",
        "        #initialize models\"\n",
        "        self.skim_gram = SkimGram(self.embedding_dims,len(self.vocabulary))\n",
        "        self.model = ConvolutionalNetwork()\n",
        "\n",
        "    def continuous_learning(self):\n",
        "        A_matrix=[]\n",
        "        count=0\n",
        "        B_matrix=[]\n",
        "        for (X_train, y_train) in zip(*(self.input_Train_conv,self.output_Train_conv)):\n",
        "            count+=1\n",
        "\n",
        "            inputX = self.data_Representation(X_train)\n",
        "\n",
        "            if count==1:\n",
        "                A_matrix=inputX.reshape(-1,516*5)\n",
        "            else:\n",
        "                A_matrix= torch.cat((A_matrix, inputX.reshape(-1,516*5)), 0)\n",
        "\n",
        "            B_matrix.append(y_train)\n",
        "\n",
        "        A = A_matrix.data.numpy()\n",
        "        u, sigma, v = np.linalg.svd(A, full_matrices=True)\n",
        "        Ui = np.linalg.norm(u,2,axis=1)\n",
        "        randm=np.random.choice(len(Ui), len(Ui), replace=False).argsort()\n",
        "        temp=[self.input_Train_conv[i] for i in randm]\n",
        "        self.input_Train_conv=temp\n",
        "        temp=[self.output_Train_conv[i] for i in randm]\n",
        "        self.output_Train_conv=temp\n",
        "  \n",
        "    def accuracy(self,test_data, model):\n",
        "        #takes data and model as input\"\n",
        "      # Testing the model and returning the accuracy on the given dataset\n",
        "        total = 0\n",
        "        correct = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for (data, target)in (test_data):\n",
        "                x = Variable(self.get_input_layer(data)).float()\n",
        "                y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
        "\n",
        "                y_pred = model(x)\n",
        "\n",
        "                total += 1\n",
        "\n",
        "                pred = y_pred.data.max()\n",
        "                correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "        return float(correct) / total \n",
        "    def accuracy_conv(self,test_data, model):\n",
        "        #takes data and model as input\"\n",
        "        total = 0\n",
        "        correct = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for (data, target)in zip(*(test_data[0],test_data[1])):\n",
        "                inputX = self.data_Representation(data)\n",
        "                y_true = torch.tensor(target, dtype=torch.float) \n",
        "                y_pred = model(inputX.view(1,1,inputX.shape[0],inputX.shape[1]))\n",
        "\n",
        "                total += 1\n",
        "\n",
        "                pred = y_pred.data.numpy()[0][0]\n",
        "                if pred>=0.5:\n",
        "                    pred=1\n",
        "                else:\n",
        "                    pred=0\n",
        "                correct +=(pred==target)\n",
        "\n",
        "        return float(correct) / total \n",
        "\n",
        "    def training_skim_gram(self, learning_rate=0.001,bts=1, num_epochs=3):\n",
        "\n",
        "        testloader = DataLoader( TensorDataset(torch.Tensor(self.input_Test),torch.Tensor(self.output_Test)), batch_size=bts, shuffle=False)\n",
        "\n",
        "        trainloader = DataLoader(TensorDataset(torch.Tensor(self.input_Train),torch.Tensor(self.output_Train)), batch_size=bts, shuffle=True)\n",
        "        optimizer = torch.optim.SGD(self.skim_gram.parameters(), lr=learning_rate)  \n",
        "      # Training skim gram\n",
        "        self.epoch_accuracies_SG = []\n",
        "        self.epoch_model_SG = []\n",
        "        self.losses_SG=[]\n",
        "        best_epoch = 0\n",
        "        best_accuracy = 0.0\n",
        "        \n",
        "        for epoch in range(num_epochs):\n",
        "            for (data, target)in trainloader:\n",
        "\n",
        "                x = Variable( self.get_input_layer(data) ).float()\n",
        "                y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
        "\n",
        "                  # Forward + Backward + Optimize\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                y_pred = self.skim_gram(x)\n",
        "                loss = F.nll_loss(y_pred.view(1,-1), y_true)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            self.losses_SG.append(loss)\n",
        "            self.epoch_accuracies_SG.append(self.accuracy(testloader, self.skim_gram))\n",
        "            self.epoch_model_SG.append(copy.copy(self.skim_gram))\n",
        "            if self.epoch_accuracies_SG[-1] > best_accuracy: #early stopping implementation\n",
        "                best_accuracy = self.epoch_accuracies_SG[-1] \n",
        "                best_epoch = epoch\n",
        "                torch.save(self.skim_gram.state_dict(), 'skim_gram.pt') #save best weight values\n",
        "\n",
        "          #early stopping here   \n",
        "            if (len(self.epoch_accuracies_SG)>10 and self.epoch_accuracies_SG[epoch-9]>self.epoch_accuracies_SG[-1]):\n",
        "                break\n",
        "\n",
        "       # self.skim_gram.load_state_dict(torch.load('skim_gram.pt')) #load best weight values\n",
        "        return best_accuracy, best_epoch \n",
        "  \n",
        "    def data_Representation(self,x):\n",
        "        \n",
        "        #document entry, reproduce document based on skim gram representation to produce embedding for the classification net\"\n",
        "        x=list(x)\n",
        "        if x:\n",
        "            with torch.no_grad():\n",
        "                first_datapoint = x.pop(0)\n",
        "                xinput= Variable(self.get_input_layer(torch.Tensor(first_datapoint))).float()\n",
        "                doc = self.skim_gram.representation(xinput)\n",
        "                doc = doc.reshape(1,-1)\n",
        "                x=np.array(x)\n",
        "            for data in x:\n",
        "                xinput= Variable(self.get_input_layer(torch.Tensor(data))).float()\n",
        "                representation = self.skim_gram.representation(xinput)\n",
        "                doc = torch.cat((doc, representation.view(1,-1)),0)\n",
        "            return doc\n",
        "        else:\n",
        "      \n",
        "          return torch.zeros(25,25) \n",
        "    \n",
        "    def training_ConvolutionNet(self,learning_rate=0.001, num_epochs=15):\n",
        "        #document entry, reproduce document based on skim gram representation to produce embedding for the classification net\"\n",
        "        epochs = num_epochs\n",
        "        train_losses = []\n",
        "        test_losses = []\n",
        "\n",
        "        self.epoch_accuracies_conv = []\n",
        "        self.epoch_model_conv = []\n",
        "        self.losses_conv = []\n",
        "        best_epoch = 0\n",
        "        best_accuracy = 0.0\n",
        "        criterion = nn.BCELoss()\n",
        "        optimizer = torch.optim.SGD(self.model.parameters(), lr=0.001)\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "          # Run the training \n",
        "          for (X_train, y_train) in zip(*(self.input_Train_conv,self.output_Train_conv)):\n",
        "\n",
        "            inputX = self.data_Representation(X_train)\n",
        "            y_train = torch.tensor(y_train, dtype=torch.float) \n",
        "            y_pred = self.model(inputX.view(1,1,inputX.shape[0],inputX.shape[1]))\n",
        "\n",
        "            loss = criterion(y_pred, y_train )\n",
        "\n",
        "            # Update parameters\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "          self.losses_conv.append(loss)\n",
        "          self.epoch_accuracies_conv.append(self.accuracy_conv([self.input_Test_conv,self.output_Test_conv], self.model))\n",
        "          self.epoch_model_conv.append(copy.copy(self.model))\n",
        "          if self.epoch_accuracies_conv[-1] > best_accuracy: #early stopping implementation\n",
        "              best_accuracy = self.epoch_accuracies_conv[-1]\n",
        "              best_epoch = epoch\n",
        "              torch.save(self.skim_gram.state_dict(), 'conv_model.pt')#save best weight values\n",
        "                # Print interim results\n",
        "          if (len(self.epoch_accuracies_conv)>10 and self.epoch_accuracies_conv[epoch-9]>self.epoch_accuracies_conv[-1]):\n",
        "              break\n",
        "          train_losses.append(loss)\n",
        "\n",
        "        #self.skim_gram.load_state_dict(torch.load('conv_model.pt'))#load best weight values\n",
        "        return best_accuracy, best_epoch\n",
        "  \n",
        "    def prediction(self,test_data,model):\n",
        "        #perform prediction on new data points\"\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        output = []\n",
        "        with torch.no_grad():\n",
        "            for k,data in enumerate(test_data):\n",
        "                for j in data:\n",
        "                    \n",
        "                    inputX = self.data_Representation(j)\n",
        "\n",
        "                    y_pred = model(inputX.view(1,1,inputX.shape[0],inputX.shape[1]))\n",
        "\n",
        "                    pred = y_pred.data.numpy()[0][0]\n",
        "\n",
        "                    if pred>=0.5:\n",
        "                        pred=1\n",
        "                    else:\n",
        "                        pred=0\n",
        "                    output.append(pred)\n",
        "        return output\n",
        "    def Test_on_New_Data(self):\n",
        "        #perform validation on new data points\"\n",
        "        check = self.accuracy_conv([ self.X_new, self.y_new],self.model)\n",
        "        if check<self.threshold:\n",
        "            print('Need to retrain')\n",
        "        return check\n",
        "    def Predict(self):\n",
        "    \n",
        "    #perform prediction on new data points\"\n",
        "        return self.prediction( self.X_new,self.model)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "noXIMgALQpEO",
        "colab": {}
      },
      "source": [
        "#set threshold value in the class as input\n",
        "architecture = Method()\n",
        "architecture.Reading('uspto.json')\n",
        "architecture.preprocess_input()\n",
        "architecture.define_models()\n",
        "architecture.Segment()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "otj2wAvZwpw3",
        "colab": {}
      },
      "source": [
        "architecture.training_skim_gram()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-_8hqmoFnQbd",
        "colab": {}
      },
      "source": [
        "architecture.losses_SG"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9VBXC26FuSCf",
        "colab": {}
      },
      "source": [
        "architecture.epoch_model_SG"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "we0k2M3YqmXU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "architecture.continuous_learning()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nQxeVq-f_aZM",
        "colab": {}
      },
      "source": [
        "architecture.training_ConvolutionNet(learning_rate=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1NNCVYgoKt0V",
        "colab": {}
      },
      "source": [
        "architecture.losses_conv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sON9LATSKt2a",
        "colab": {}
      },
      "source": [
        "architecture.epoch_accuracies_conv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P0JAgAb7HxtL",
        "colab": {}
      },
      "source": [
        "plt.plot(range(len(architecture.losses_SG)), architecture.losses_SG)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.title('Training  Loss vs epoch for skim-gram model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OpEcRhjLt4fx",
        "colab": {}
      },
      "source": [
        "plt.plot(range(len(architecture.losses_conv)), architecture.losses_conv)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.title('Training Loss vs epoch for convonlutional neural net model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bSGJaiUbb_Z_",
        "colab": {}
      },
      "source": [
        "plt.plot(range(len(architecture.losses_conv)), architecture.epoch_accuracies_conv)\n",
        "plt.ylabel('Test Accuracy ')\n",
        "plt.xlabel('epoch')\n",
        "plt.title('Test Accuracy (in scale 0-1) vs epoch for convonlutional neural net model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8VDonnrTHq6c"
      },
      "source": [
        "***How to call API for new data validation(provides labels) or prediction(no labels)***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bSONJ2edSK5e",
        "colab": {}
      },
      "source": [
        "architecture.New_Data_Read('uspto.json')#for validation data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dw-MnbjAJJAX",
        "colab": {}
      },
      "source": [
        "architecture.Test_on_New_Data() #validation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jgnrbpWBHnSd",
        "colab": {}
      },
      "source": [
        "architecture.New_Data_Read('uspto.json',FLAG=1)#for predictive data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hUSQR6K0Haen",
        "colab": {}
      },
      "source": [
        "architecture.Predict()#predict on new data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E2M1Tah6KJPq",
        "colab": {}
      },
      "source": [
        "architecture.df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uJlgNQktNWX1",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}